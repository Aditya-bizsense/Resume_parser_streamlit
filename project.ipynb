{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Information:\n",
      "Here's the extracted information in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Charles McTurland\",\n",
      "  \"contact\": {\n",
      "    \"email\": \"cmcturland@email.com\",\n",
      "    \"phone\": \"(123) 456-7890\",\n",
      "    \"linkedin\": \"linkedin.com/in/charlesmcturland\"\n",
      "  },\n",
      "  \"skills\": [\n",
      "    \"Python (Django)\",\n",
      "    \"Javascript (NodeJS, ReactJS, jQuery)\",\n",
      "    \"SQL (MySQL, PostgreSQL, NoSQL)\",\n",
      "    \"HTML5/CSS\",\n",
      "    \"AWS\",\n",
      "    \"Unix\",\n",
      "    \"Git\"\n",
      "  ],\n",
      "  \"education\": {\n",
      "    \"degree\": \"B.S. Computer Science\",\n",
      "    \"university\": \"University of Pittsburgh\",\n",
      "    \"duration\": \"September 2008 - April 2012\"\n",
      "  },\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"company\": \"Embark\",\n",
      "      \"position\": \"Software Engineer\",\n",
      "      \"duration\": \"January 2015 - current\",\n",
      "      \"location\": \"New York, NY\"\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"MarketSmart\",\n",
      "      \"position\": \"Software Engineer\",\n",
      "      \"duration\": \"April 2012 - January 2015\",\n",
      "      \"location\": \"Washington, DC\"\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"Marketing Science Company\",\n",
      "      \"position\": \"Software Engineer Intern\",\n",
      "      \"duration\": \"April 2011 - March 2012\",\n",
      "      \"location\": \"Pittsburgh, PA\"\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Poker Simulation\",\n",
      "      \"description\": \"Built a full-stack web app to simulate and visualize outcomes of poker hands against opponents of different play styles\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Note: I have only extracted the key points from education and experience sections as per the request, discarding excess information. Also, I assumed the LinkedIn profile URL as it was not provided in the resume. If you need any modifications or have further questions, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq client with the API key\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "\n",
    "def extract_entities_with_groq(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that extracts structured data from resumes. Output should be in JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract key information (like name, contact, skills, education, projects and experience), discarding excess info about education and experience, from the following resume:\\n{text}\"}\n",
    "        ],\n",
    "        model=\"llama-3.3-70b-versatile\",  # Replace with the Groq model you want to use\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "pdf_path = input(\"Enter resume path: \")  # Replace with your resume file\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: File '{pdf_path}' not found.\")\n",
    "else:\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    if not resume_text.strip():\n",
    "            print(\"Error: No text extracted from the PDF. Check the file content.\")\n",
    "    else:\n",
    "        # Extract entities with Groq\n",
    "        extracted_info = extract_entities_with_groq(resume_text)\n",
    "            \n",
    "        # Print extracted info\n",
    "        print(\"Extracted Information:\")\n",
    "        print(extracted_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving to ChromaDB: 'str' object has no attribute 'get'\n",
      "Saved Information in JSON file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_resume.db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"resumes\")\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq client with the API key\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Error: GROQ_API_KEY is missing. Please set it in your .env file.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "            if text.strip():\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\".join(page.get_text() for page in doc) # type: ignore\n",
    "        if text.strip():\n",
    "            return text \n",
    "    except Exception as e:\n",
    "        print(f\"fitz (PyMuPDF) failed: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_entities_with_groq(text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI that extracts key information from resumes. Summarize the candidate's essential details in a few sentences, including their name, contact, skills, education, and job titles. Do not give anything else as output.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract key information (like name, contact, skills, education, projects, certifications, and experience) from the following resume:\\n{text}\"}\n",
    "            ],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Groq API: {e}\")\n",
    "    return None\n",
    "\n",
    "# def clean_llm_output(llm_output):\n",
    "#     # Remove backticks and \"```json\" if present\n",
    "#     cleaned_output = re.sub(r\"```(json)?\", \"\", llm_output).strip()\n",
    "#     return cleaned_output\n",
    "\n",
    "# def convert_llm_output_to_dict(llm_output):\n",
    "#     try:\n",
    "#         cleaned_output = clean_llm_output(llm_output)\n",
    "#         return json.loads(cleaned_output)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Error decoding JSON: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "def save_to_chromadb(resume_data, raw_text):\n",
    "    try:\n",
    "        # Add resume data to Chroma collection\n",
    "        collection.add(\n",
    "            documents=[raw_text],  # Store the raw resume text\n",
    "            metadatas=[{\"summary\":resume_data}],  # Store structured data as metadata\n",
    "            ids=[resume_data.get(\"name\", \"unknown\").replace(\" \", \"_\").lower()]\n",
    "        )\n",
    "        print(\"Resume data saved to ChromaDB\")\n",
    "    # Save everything back to the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to ChromaDB: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = input(\"Enter resume path: \")\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' not found.\")\n",
    "    else:\n",
    "        resume_text = extract_text_with_pdfplumber(pdf_path)\n",
    "        \n",
    "        if not resume_text:\n",
    "            print(\"pdfplumber extraction failed or returned empty text. Trying fitz...\")\n",
    "            resume_text = extract_text_with_fitz(pdf_path)\n",
    "        \n",
    "        if not resume_text:\n",
    "            print(\"Error: No text extracted from the PDF. Check the file content.\")\n",
    "        else:\n",
    "            # Extract entities with Groq\n",
    "            extracted_info = extract_entities_with_groq(resume_text)\n",
    "            # structured_data = convert_llm_output_to_dict(extracted_info)\n",
    "            if extracted_info:\n",
    "                save_to_chromadb(extracted_info, resume_text)\n",
    "                print(\"Saved Information in JSON file\")\n",
    "                # try:\n",
    "                #     # Try to parse the result as JSON\n",
    "                #     parsed_info = json.loads(extracted_info)\n",
    "                #     print(\"Extracted Information:\")\n",
    "                #     print(json.dumps(parsed_info, indent=4))\n",
    "                # except json.JSONDecodeError:\n",
    "                #     # Handle non-JSON output\n",
    "                #     print(\"Extracted Information (raw):\")\n",
    "                #     print(extracted_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2', 'id3', 'id4', 'id1']], 'embeddings': None, 'documents': [['This is a pineapple', 'This is an Apple', 'This is a book', 'This is a pen']], 'uris': None, 'data': None, 'metadatas': [[None, None, None, None]], 'distances': [[1.2466824285124267, 1.524765415432772, 1.5835720648468317, 1.8873233167385282]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"chroma.db\")\n",
    "\n",
    "collection = client.create_collection(name=\"food\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"This is a pen\",\n",
    "        \"This is a pineapple\",\n",
    "        \"This is an Apple\",\n",
    "        \"This is a book\"\n",
    "    ],\n",
    "    ids=[\n",
    "        \"id1\",\n",
    "        \"id2\",\n",
    "        \"id3\",\n",
    "        \"id4\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query about food\"], # Chroma will embed this for you\n",
    "    n_results=4 # how many results to return\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: id4\n",
      "Text: This is a book\n",
      "Distance: 1.4273\n",
      "--------------------\n",
      "Document ID: id1\n",
      "Text: This is a pen\n",
      "Distance: 1.6124\n",
      "--------------------\n",
      "Document ID: id3\n",
      "Text: This is an Apple\n",
      "Distance: 1.7944\n",
      "--------------------\n",
      "Document ID: id2\n",
      "Text: This is a pineapple\n",
      "Distance: 1.8213\n",
      "--------------------\n",
      "{'ids': [['id4', 'id1', 'id3', 'id2']], 'embeddings': None, 'documents': [['This is a book', 'This is a pen', 'This is an Apple', 'This is a pineapple']], 'uris': None, 'data': None, 'metadatas': [[None, None, None, None]], 'distances': [[1.4272642063675842, 1.612399273619461, 1.7944306332278257, 1.8212716796584583]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"chroma.db\")\n",
    "collection = client.get_collection(name=\"food\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query about studying\"], # Chroma will embed this for you\n",
    "    n_results=4, # how many results to return\n",
    ")\n",
    "\n",
    "# Extract results safely\n",
    "ids = results.get('ids', [[]])[0] or []\n",
    "documents = results.get('documents', [[]])[0] or [] # type: ignore\n",
    "distances = results.get('distances', [[]])[0] or [] # type: ignore\n",
    "\n",
    "# Print the results\n",
    "for i in range(len(ids)):\n",
    "    print(f\"Document ID: {ids[i]}\")\n",
    "    print(f\"Text: {documents[i]}\")\n",
    "    print(f\"Distance: {distances[i]:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"name\": \"Janine Nel\",\n",
      "  \"contact\": {\n",
      "    \"address\": \"1515 Pacific Ave, Los Angeles, CA 90291, United States\",\n",
      "    \"phone\": \"3868683442\",\n",
      "    \"email\": \"email@email.com\"\n",
      "  },\n",
      "  \"skills\": [\n",
      "    \"AutoCAD\",\n",
      "    \"Industry Trends & Sales Forecasting\",\n",
      "    \"Knowledge of Technical Diagrams\",\n",
      "    \"Engineering\",\n",
      "    \"Agile Project Management\",\n",
      "    \"English\",\n",
      "    \"Dutch\"\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"Masters in Industrial Engineering\",\n",
      "      \"institution\": \"Harvard University\",\n",
      "      \"location\": \"Miami\",\n",
      "      \"date\": \"January 2019 — May 2022\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Professional Engineering (PE) Exam\",\n",
      "      \"institution\": \"National Council of Examiners for Engineering and Surveying (NCEES)\",\n",
      "      \"location\": \"Newton\",\n",
      "      \"date\": \"January 2018 — December 2019\"\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"certification\": \"Certified Associate in Project Management (CAPM)\",\n",
      "      \"institution\": \"Project Management Institute (PMI)\",\n",
      "      \"location\": \"Seneca, South Carolina\",\n",
      "      \"date\": \"May 2021 — May 2021\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"jobTitle\": \"Sales Engineer\",\n",
      "      \"company\": \"Engen Oil\",\n",
      "      \"location\": \"Jacksonville\",\n",
      "      \"date\": \"May 2022 — May 2022\"\n",
      "    },\n",
      "    {\n",
      "      \"jobTitle\": \"Sales Engineer\",\n",
      "      \"company\": \"Quest Medical\",\n",
      "      \"location\": \"Los Angeles\",\n",
      "      \"date\": \"January 2019 — April 2021\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Summary data for 'janine_nel' saved to ChromaDB\n",
      "Saved Information to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_resume.db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"resumes\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq client with the API key\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Error: GROQ_API_KEY is missing. Please set it in your .env file.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "            if text.strip():\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_text_with_fitz(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\".join(page.get_text() for page in doc)  # type: ignore\n",
    "        if text.strip():\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"fitz (PyMuPDF) failed: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_entities_with_groq(text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI that extracts structured data from resumes. Output should be in JSON format only. Exclude descriptions and work done in job. Do not give anything else as output.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract key information (like name, contact, skills, education, projects, certifications, and experience) from the following resume:\\n{text}\"}\n",
    "            ],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Groq API: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def convert_llm_output_to_dict(llm_output):\n",
    "    try:\n",
    "        cleaned_output = re.sub(r\"```(json)?\", \"\", llm_output).strip()\n",
    "        return json.loads(cleaned_output)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_chromadb(resume_data):\n",
    "    try:\n",
    "        if not resume_data:\n",
    "            print(\"No resume data to save.\")\n",
    "            return\n",
    "        \n",
    "        resume_dict = convert_llm_output_to_dict(resume_data)\n",
    "        if not resume_dict:\n",
    "            print(\"Failed to parse LLM output to JSON.\")\n",
    "            return\n",
    "\n",
    "        candidate_name = resume_dict.get(\"name\", \"unknown\").replace(\" \", \"_\").lower()\n",
    "        summary = json.dumps(resume_dict)\n",
    "        \n",
    "        collection.add(\n",
    "            documents=[summary],  # Store only the summarized data\n",
    "            metadatas=[{\"name\": resume_dict.get(\"name\", \"unknown\")}],  # Minimal metadata\n",
    "            ids=[candidate_name]\n",
    "        )\n",
    "        print(f\"Summary data for '{candidate_name}' saved to ChromaDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to ChromaDB: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = input(\"Enter resume path: \")\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' not found.\")\n",
    "    else:\n",
    "        resume_text = extract_text_with_pdfplumber(pdf_path)\n",
    "        \n",
    "        if not resume_text:\n",
    "            print(\"pdfplumber extraction failed or returned empty text. Trying fitz...\")\n",
    "            resume_text = extract_text_with_fitz(pdf_path)\n",
    "        \n",
    "        if not resume_text:\n",
    "            print(\"Error: No text extracted from the PDF. Check the file content.\")\n",
    "        else:\n",
    "            # Extract entities with Groq\n",
    "            extracted_info = extract_entities_with_groq(resume_text)\n",
    "            print(extracted_info)\n",
    "            if extracted_info:\n",
    "                save_to_chromadb(extracted_info)\n",
    "                print(\"Saved Information to ChromaDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: janine_nel\n",
      "Distance: 1.5689\n",
      "--------------------\n",
      "Name: charles_mcturland\n",
      "Distance: 1.6711\n",
      "--------------------\n",
      "Name: cynthia_dwayne\n",
      "Distance: 1.7637\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"chroma_resume.db\")\n",
    "collection = client.get_collection(name=\"resumes2\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"who is the most experienced\"], # Chroma will embed this for you\n",
    "    n_results=5, # how many results to return\n",
    ")\n",
    "\n",
    "ids = results.get('ids', [[]])[0] or []\n",
    "distances = results.get('distances', [[]])[0] or [] # type: ignore\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    # print(f\"Document ID: {i+1}\")\n",
    "    print(f\"Name: {ids[i]}\")\n",
    "    print(f\"Distance: {distances[i]:.4f}\")\n",
    "    print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data for 'kristen_connelly' saved to ChromaDB\n",
      "Saved Information to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_resume.db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"resumes3\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Error: GROQ_API_KEY is missing. Please set it in your .env file.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "            if text.strip():\n",
    "                return text\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed: {e}\")\n",
    "    return None\n",
    "\n",
    "def LLM(text):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create( \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI that summarizes resumes. 'Name:' should be used before person's name. Print skills as it is. Mention time frame of experience along with roles in experience.  Do not give descriptions. Do not exceed 2 lines for any given section.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize and Extract key information (like personal details, skills, education, projects, certifications, experience and certifications(optional)) from the following resume:\\n{text}\"}\n",
    "            ],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Groq API: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_to_chromadb(resume_data):\n",
    "    try:\n",
    "        if not resume_data:\n",
    "            print(\"No resume data to save.\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        name_match = re.search(r\"Name:\\s*(.*)\", resume_data)\n",
    "        candidate_name = name_match.group(1).strip().replace(\" \", \"_\").lower() if name_match else \"unknown\"\n",
    "        \n",
    "        collection.add(\n",
    "            documents=[resume_data],  # Store summarized data\n",
    "            metadatas=[{\"name\": candidate_name}],  # Store the extracted name in metadata\n",
    "            ids=[candidate_name]\n",
    "        )\n",
    "        print(f\"Summary data for '{candidate_name}' saved to ChromaDB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to ChromaDB: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pdf_path = input(\"Enter path: \")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Error: File '{pdf_path}' not found.\")\n",
    "else:\n",
    "    resume_text = extract_text_with_pdfplumber(pdf_path)\n",
    "    if not resume_text:\n",
    "        print(\"Error: No text extracted from the PDF. Check the file content.\")\n",
    "    else:\n",
    "        # Extract entities with Groq\n",
    "        extracted_info = LLM(resume_text)\n",
    "        if extracted_info:\n",
    "            #print(extracted_info)\n",
    "            save_to_chromadb(extracted_info)\n",
    "            print(\"Saved Information to ChromaDB\")\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['kristen_connelly', 'cynthia_dwayne', 'charles_mcturland']], 'embeddings': [array([[-0.09390237, -0.09990636,  0.00696741, ..., -0.0504648 ,\n",
      "         0.02070054,  0.05681674],\n",
      "       [-0.0889769 , -0.05071229,  0.0372017 , ..., -0.01357653,\n",
      "        -0.0081168 ,  0.05889499],\n",
      "       [-0.05332222, -0.01952961, -0.00477163, ..., -0.07719181,\n",
      "        -0.04477137,  0.10925198]], shape=(3, 384))], 'documents': [['Name: Kristen Connelly\\nContact: 3868683442, email@email.com, 1515 Pacific Ave, Los Angeles, CA 90291\\n\\nSkills: \\nAdobe Premiere Pro, Call Sheets & Sides, Camera Boom, Light Boom, Mic Boom, DaVinci Resolve, Languages: English, Dutch; Flemish\\n\\nEducation: \\nBA in Film and Television, Boston University, FEBRUARY 2021 — PRESENT, Advanced Course in Digital Video Editing, ADMEC Multimedia Institute, JANUARY 2018 — JULY 2018\\n\\nExperience: \\nVideo Production Assistant, Blue Penguin Designs, FEBRUARY 2021 — PRESENT, Video Production Assistant, Botle Bob Advertising, JANUARY 2019 — FEBRUARY 2021\\n\\nCertifications: \\nHootsuite Certified Professional, Hootsuite Media, JANUARY 2020 — FEBRUARY 2021, Adobe CS5 Certified, University of Delaware, JANUARY 2020 — DECEMBER 2020', 'Name: Cynthia Dwayne\\nContact: cynthia@beamjobs.com, (123) 456-7890, Brooklyn, NY\\n\\nSkills: Python (Django), SQL (PostgreSQL, MySQL), Cloud (GCP, AWS), JavaScript (ES6, React, Redux, Node.js), Typescript, HTML/ CSS, CI/CD\\n\\nEducation: Bachelor of Science Computer Science, University of Delaware, August 2008 - May 2012\\n\\nExperience: \\nSoftware Developer, January 2017 - current, New York, NY\\nFront-End Developer, January 2014 - December 2016, New York, NY\\nHelp Desk Analyst, June 2012 - January 2014, New York, NY', 'Name: Charles McTurland\\nSkills: Python (Django), Javascript (NodeJS ReactJS, jQuery), SQL (MySQL, PostgreSQL, NoSQL), HTML5/CSS, AWS, Unix, Git\\n\\nEducation: B.S. Computer Science, University of Pittsburgh, September 2008 - April 2012\\n\\nExperience: \\nSoftware Engineer, Embark, January 2015 - current \\nSoftware Engineer, MarketSmart, April 2012 - January 2015 \\nSoftware Engineer Intern, Marketing Science Company, April 2011 - March 2012 \\n\\nProjects: Poker Simulation \\nCertifications: None mentioned']], 'uris': None, 'data': None, 'metadatas': None, 'distances': [[1.6521826830591342, 1.670692303340543, 1.6922668023922265]], 'included': [<IncludeEnum.embeddings: 'embeddings'>, <IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>]}\n"
     ]
    }
   ],
   "source": [
    "res = collection.query(\n",
    "    query_texts=[\"\"], # Chroma will embed this for you\n",
    "    n_results=5, # how many results to return\n",
    "    include=[\"embeddings\",\"documents\",\"distances\"] # type: ignore\n",
    ")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
